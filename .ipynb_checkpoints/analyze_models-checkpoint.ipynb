{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# YouTube Trending Video ML Model Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a detailed analysis of the trained machine learning models for predicting YouTube trending videos. We'll analyze model performance, feature importance, and make predictions to gain insights into what drives video popularity on YouTube.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Setup and Data Loading\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from typing import Dict, List, Any, Optional, Union, Tuple\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure matplotlib and seaborn\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8-whitegrid')\\n\",\n",
    "    \"sns.set_palette(\\\"viridis\\\")\\n\",\n",
    "    \"plt.rcParams.update({\\n\",\n",
    "    \"    'figure.figsize': (12, 8),\\n\",\n",
    "    \"    'font.size': 12,\\n\",\n",
    "    \"    'axes.titlesize': 14,\\n\",\n",
    "    \"    'axes.labelsize': 12\\n\",\n",
    "    \"})\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define helper functions for loading models and data\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_model_and_metadata(model_dir: str) -> Tuple[Any, Dict]:\\n\",\n",
    "    \"    \\\"\\\"\\\"Load a model and its metadata from the specified directory.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        model_dir: Directory containing the model files\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        Tuple of (model, metadata)\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Get model name from directory\\n\",\n",
    "    \"    model_name = os.path.basename(model_dir)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load model\\n\",\n",
    "    \"    model_path = os.path.join(model_dir, f\\\"{model_name}.pkl\\\")\\n\",\n",
    "    \"    metadata_path = os.path.join(model_dir, f\\\"{model_name}_metadata.json\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not os.path.exists(model_path):\\n\",\n",
    "    \"        raise FileNotFoundError(f\\\"Model file not found: {model_path}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not os.path.exists(metadata_path):\\n\",\n",
    "    \"        raise FileNotFoundError(f\\\"Metadata file not found: {metadata_path}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load the model\\n\",\n",
    "    \"    with open(model_path, 'rb') as f:\\n\",\n",
    "    \"        model = pickle.load(f)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load the metadata\\n\",\n",
    "    \"    with open(metadata_path, 'r') as f:\\n\",\n",
    "    \"        metadata = json.load(f)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return model, metadata\\n\",\n",
    "    \"\\n\",\n",
    "    \"def find_all_model_dirs(base_dir: str = 'models') -> Dict[str, List[str]]:\\n\",\n",
    "    \"    \\\"\\\"\\\"Find all model directories in the base directory.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        base_dir: Base directory containing model subdirectories\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        Dictionary mapping model types to lists of model directories\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    model_dirs = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Check for classification models\\n\",\n",
    "    \"    cls_dir = os.path.join(base_dir, 'classification')\\n\",\n",
    "    \"    if os.path.exists(cls_dir):\\n\",\n",
    "    \"        # Find all subdirectories in the classification directory\\n\",\n",
    "    \"        model_dirs['classification'] = [\\n\",\n",
    "    \"            os.path.join(cls_dir, d) for d in os.listdir(cls_dir)\\n\",\n",
    "    \"            if os.path.isdir(os.path.join(cls_dir, d))\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Check for regression models\\n\",\n",
    "    \"    reg_dir = os.path.join(base_dir, 'regression')\\n\",\n",
    "    \"    if os.path.exists(reg_dir):\\n\",\n",
    "    \"        # Find all subdirectories in the regression directory\\n\",\n",
    "    \"        model_dirs['regression'] = [\\n\",\n",
    "    \"            os.path.join(reg_dir, d) for d in os.listdir(reg_dir)\\n\",\n",
    "    \"            if os.path.isdir(os.path.join(reg_dir, d))\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return model_dirs\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_data(data_path=None):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load the trending video data.\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        if data_path is None:\\n\",\n",
    "    \"            # Find the most recent data file\\n\",\n",
    "    \"            data_dir = os.path.join(os.getcwd(), \\\"data/processed\\\")\\n\",\n",
    "    \"            if not os.path.exists(data_dir):\\n\",\n",
    "    \"                return None\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Find most recent trending data file\\n\",\n",
    "    \"            files = [f for f in os.listdir(data_dir) if f.startswith(\\\"all_trending_\\\")]\\n\",\n",
    "    \"            if not files:\\n\",\n",
    "    \"                return None\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            latest_file = max(files)\\n\",\n",
    "    \"            data_path = os.path.join(data_dir, latest_file)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Load the data\\n\",\n",
    "    \"        df = pd.read_csv(data_path)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Process datetime columns\\n\",\n",
    "    \"        datetime_cols = ['publish_time', 'fetch_time']\\n\",\n",
    "    \"        for col in datetime_cols:\\n\",\n",
    "    \"            if col in df.columns:\\n\",\n",
    "    \"                df[col] = pd.to_datetime(df[col], errors='coerce')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return df\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error loading data: {e}\\\")\\n\",\n",
    "    \"        return None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Find and load all models\\n\",\n",
    "    \"models_dir = 'models'  # Update this path if your models are stored elsewhere\\n\",\n",
    "    \"model_directories = find_all_model_dirs(models_dir)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Found models:\\\")\\n\",\n",
    "    \"for model_type, directories in model_directories.items():\\n\",\n",
    "    \"    print(f\\\"- {model_type}: {len(directories)} models\\\")\\n\",\n",
    "    \"    for directory in directories:\\n\",\n",
    "    \"        print(f\\\"  - {os.path.basename(directory)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load the data\\n\",\n",
    "    \"df = load_data()\\n\",\n",
    "    \"\\n\",\n",
    "    \"if df is not None:\\n\",\n",
    "    \"    print(f\\\"Loaded {len(df)} rows and {len(df.columns)} columns\\\")\\n\",\n",
    "    \"    print(f\\\"Regions: {df['region'].unique()}\\\")\\n\",\n",
    "    \"    print(f\\\"Sample columns: {list(df.columns)[:10]}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Failed to load data\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Classification Model Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def compare_classification_models(model_dirs: List[str]) -> pd.DataFrame:\\n\",\n",
    "    \"    \\\"\\\"\\\"Compare classification models based on their performance metrics.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        model_dirs: List of directories containing classification models\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        DataFrame with model performance metrics\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Initialize results list\\n\",\n",
    "    \"    results = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load and analyze each model\\n\",\n",
    "    \"    for model_dir in model_dirs:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            _, metadata = load_model_and_metadata(model_dir)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Extract model information\\n\",\n",
    "    \"            model_name = metadata.get('model_name', os.path.basename(model_dir))\\n\",\n",
    "    \"            target_name = metadata.get('target_name', 'unknown')\\n\",\n",
    "    \"            model_type = metadata.get('model_type', 'unknown')\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Extract metrics\\n\",\n",
    "    \"            metrics = metadata.get('metrics', {})\\n\",\n",
    "    \"            accuracy = metrics.get('accuracy', None)\\n\",\n",
    "    \"            precision = metrics.get('precision', None)\\n\",\n",
    "    \"            recall = metrics.get('recall', None)\\n\",\n",
    "    \"            f1 = metrics.get('f1', None)\\n\",\n",
    "    \"            auc = metrics.get('auc', None)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Add to results\\n\",\n",
    "    \"            results.append({\\n\",\n",
    "    \"                'model_name': model_name,\\n\",\n",
    "    \"                'target_name': target_name,\\n\",\n",
    "    \"                'model_type': model_type,\\n\",\n",
    "    \"                'accuracy': accuracy,\\n\",\n",
    "    \"                'precision': precision,\\n\",\n",
    "    \"                'recall': recall,\\n\",\n",
    "    \"                'f1': f1,\\n\",\n",
    "    \"                'auc': auc\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Error loading model from {model_dir}: {e}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Convert to DataFrame\\n\",\n",
    "    \"    df = pd.DataFrame(results)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sort by target and f1 score\\n\",\n",
    "    \"    df = df.sort_values(['target_name', 'f1'], ascending=[True, False])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare classification models\\n\",\n",
    "    \"if 'classification' in model_directories and model_directories['classification']:\\n\",\n",
    "    \"    classification_comparison = compare_classification_models(model_directories['classification'])\\n\",\n",
    "    \"    classification_comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize classification model performance\\n\",\n",
    "    \"if 'classification' in model_directories and model_directories['classification']:\\n\",\n",
    "    \"    # Group models by target\\n\",\n",
    "    \"    targets = classification_comparison['target_name'].unique()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for target in targets:\\n\",\n",
    "    \"        target_models = classification_comparison[classification_comparison['target_name'] == target]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create figure\\n\",\n",
    "    \"        fig, ax = plt.subplots(figsize=(10, 6))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot metrics\\n\",\n",
    "    \"        metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\\n\",\n",
    "    \"        available_metrics = [m for m in metrics if m in target_models.columns and not target_models[m].isna().all()]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        x = np.arange(len(available_metrics))\\n\",\n",
    "    \"        width = 0.8 / len(target_models)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for i, (_, row) in enumerate(target_models.iterrows()):\\n\",\n",
    "    \"            values = [row[m] for m in available_metrics]\\n\",\n",
    "    \"            ax.bar(x + i * width - width * len(target_models) / 2, values, width, label=row['model_type'])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add labels and legend\\n\",\n",
    "    \"        ax.set_ylabel('Score')\\n\",\n",
    "    \"        ax.set_title(f'Performance Metrics for {target} Models')\\n\",\n",
    "    \"        ax.set_xticks(x)\\n\",\n",
    "    \"        ax.set_xticklabels(available_metrics)\\n\",\n",
    "    \"        ax.legend()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Set y-axis limits\\n\",\n",
    "    \"        ax.set_ylim(0, 1.0)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add grid\\n\",\n",
    "    \"        ax.grid(axis='y', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Regression Model Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def compare_regression_models(model_dirs: List[str]) -> pd.DataFrame:\\n\",\n",
    "    \"    \\\"\\\"\\\"Compare regression models based on their performance metrics.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        model_dirs: List of directories containing regression models\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        DataFrame with model performance metrics\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Initialize results list\\n\",\n",
    "    \"    results = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load and analyze each model\\n\",\n",
    "    \"    for model_dir in model_dirs:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            _, metadata = load_model_and_metadata(model_dir)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Extract model information\\n\",\n",
    "    \"            model_name = metadata.get('model_name', os.path.basename(model_dir))\\n\",\n",
    "    \"            target_name = metadata.get('target_name', 'unknown')\\n\",\n",
    "    \"            model_type = metadata.get('model_type', 'unknown')\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Extract metrics\\n\",\n",
    "    \"            metrics = metadata.get('metrics', {})\\n\",\n",
    "    \"            mse = metrics.get('mse', None)\\n\",\n",
    "    \"            rmse = metrics.get('rmse', None)\\n\",\n",
    "    \"            mae = metrics.get('mae', None)\\n\",\n",
    "    \"            r2 = metrics.get('r2', None)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Add to results\\n\",\n",
    "    \"            results.append({\\n\",\n",
    "    \"                'model_name': model_name,\\n\",\n",
    "    \"                'target_name': target_name,\\n\",\n",
    "    \"                'model_type': model_type,\\n\",\n",
    "    \"                'mse': mse,\\n\",\n",
    "    \"                'rmse': rmse,\\n\",\n",
    "    \"                'mae': mae,\\n\",\n",
    "    \"                'r2': r2\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Error loading model from {model_dir}: {e}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Convert to DataFrame\\n\",\n",
    "    \"    df = pd.DataFrame(results)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sort by target and R² score\\n\",\n",
    "    \"    df = df.sort_values(['target_name', 'r2'], ascending=[True, False])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare regression models\\n\",\n",
    "    \"if 'regression' in model_directories and model_directories['regression']:\\n\",\n",
    "    \"    regression_comparison = compare_regression_models(model_directories['regression'])\\n\",\n",
    "    \"    regression_comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize regression model performance\\n\",\n",
    "    \"if 'regression' in model_directories and model_directories['regression']:\\n\",\n",
    "    \"    # Group models by target\\n\",\n",
    "    \"    targets = regression_comparison['target_name'].unique()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for target in targets:\\n\",\n",
    "    \"        target_models = regression_comparison[regression_comparison['target_name'] == target]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create figure for R² and error metrics\\n\",\n",
    "    \"        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot R² score (higher is better)\\n\",\n",
    "    \"        model_types = target_models['model_type'].tolist()\\n\",\n",
    "    \"        r2_scores = target_models['r2'].tolist()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        ax1.bar(model_types, r2_scores, color='green', alpha=0.7)\\n\",\n",
    "    \"        ax1.set_title(f'R² Score for {target} Models')\\n\",\n",
    "    \"        ax1.set_ylabel('R² Score')\\n\",\n",
    "    \"        ax1.set_ylim(0, 1.0)\\n\",\n",
    "    \"        ax1.grid(axis='y', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot error metrics (lower is better)\\n\",\n",
    "    \"        metrics = ['rmse', 'mae']\\n\",\n",
    "    \"        available_metrics = [m for m in metrics if m in target_models.columns and not target_models[m].isna().all()]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for metric in available_metrics:\\n\",\n",
    "    \"            values = target_models[metric].tolist()\\n\",\n",
    "    \"            ax2.bar(model_types, values, alpha=0.7, label=metric.upper())\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        ax2.set_title(f'Error Metrics for {target} Models')\\n\",\n",
    "    \"        ax2.set_ylabel('Error')\\n\",\n",
    "    \"        ax2.legend()\\n\",\n",
    "    \"        ax2.grid(axis='y', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Feature Importance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def analyze_feature_importance(model_dir: str) -> pd.DataFrame:\\n\",\n",
    "    \"    \\\"\\\"\\\"Analyze feature importance for a model.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        model_dir: Directory containing the model files\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        DataFrame with feature importances\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # Load model and metadata\\n\",\n",
    "    \"        model, metadata = load_model_and_metadata(model_dir)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Extract feature importances\\n\",\n",
    "    \"        importances = metadata.get('feature_importances', {})\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if not importances:\\n\",\n",
    "    \"            print(f\\\"No feature importances found for {os.path.basename(model_dir)}\\\")\\n\",\n",
    "    \"            return pd.DataFrame()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert to DataFrame\\n\",\n",
    "    \"        importance_df = pd.DataFrame({\\n\",\n",
    "    \"            'Feature': list(importances.keys()),\\n\",\n",
    "    \"            'Importance': list(importances.values())\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Sort by importance\\n\",\n",
    "    \"        importance_df = importance_df.sort_values('Importance', ascending=False)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return importance_df\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error analyzing feature importance for {model_dir}: {e}\\\")\\n\",\n",
    "    \"        return pd.DataFrame()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Select a model for feature importance analysis\\n\",\n",
    "    \"# Let's look at one classification and one regression model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classification model (viral prediction)\\n\",\n",
    "    \"viral_model_dirs = [d for d in model_directories.get('classification', []) if 'viral' in os.path.basename(d)]\\n\",\n",
    "    \"if viral_model_dirs:\\n\",\n",
    "    \"    viral_model_dir = viral_model_dirs[0]  # Use the first matching model\\n\",\n",
    "    \"    print(f\\\"Analyzing feature importance for {os.path.basename(viral_model_dir)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    viral_importances = analyze_feature_importance(viral_model_dir)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not viral_importances.empty:\\n\",\n",
    "    \"        # Display top features\\n\",\n",
    "    \"        top_n = 20\\n\",\n",
    "    \"        top_features = viral_importances.head(top_n)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot\\n\",\n",
    "    \"        plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"        sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\\n\",\n",
    "    \"        plt.title(f'Top {top_n} Features for Viral Prediction')\\n\",\n",
    "    \"        plt.xlabel('Importance')\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Display table\\n\",\n",
    "    \"        viral_importances.head(top_n)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Regression model (engagement prediction)\\n\",\n",
    "    \"engagement_model_dirs = [d for d in model_directories.get('regression', []) if 'engagement' in os.path.basename(d)]\\n\",\n",
    "    \"if engagement_model_dirs:\\n\",\n",
    "    \"    engagement_model_dir = engagement_model_dirs[0]  # Use the first matching model\\n\",\n",
    "    \"    print(f\\\"Analyzing feature importance for {os.path.basename(engagement_model_dir)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    engagement_importances = analyze_feature_importance(engagement_model_dir)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not engagement_importances.empty:\\n\",\n",
    "    \"        # Display top features\\n\",\n",
    "    \"        top_n = 20\\n\",\n",
    "    \"        top_features = engagement_importances.head(top_n)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot\\n\",\n",
    "    \"        plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"        sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\\n\",\n",
    "    \"        plt.title(f'Top {top_n} Features for Engagement Prediction')\\n\",\n",
    "    \"        plt.xlabel('Importance')\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Display table\\n\",\n",
    "    \"        engagement_importances.head(top_n)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Compare Feature Importance Across Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def compare_feature_importance(model_dirs: List[str], top_n: int = 10) -> pd.DataFrame:\\n\",\n",
    "    \"    \\\"\\\"\\\"Compare feature importance across multiple models.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        model_dirs: List of model directories\\n\",\n",
    "    \"        top_n: Number of top features to include\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        DataFrame with feature importances for each model\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    all_importances = {}\\n\",\n",
    "    \"    model_names = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for model_dir in model_dirs:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            # Load model and metadata\\n\",\n",
    "    \"            _, metadata = load_model_and_metadata(model_dir)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Get model name and target\\n\",\n",
    "    \"            model_name = metadata.get('model_name', os.path.basename(model_dir))\\n\",\n",
    "    \"            target_name = metadata.get('target_name', 'unknown')\\n\",\n",
    "    \"            model_type = metadata.get('model_type', 'unknown')\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Combine info for display\\n\",\n",
    "    \"            display_name = f\\\"{target_name} ({model_type})\\\"\\n\",\n",
    "    \"            model_names.append(display_name)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Extract feature importances\\n\",\n",
    "    \"            importances = metadata.get('feature_importances', {})\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if importances:\\n\",\n",
    "    \"                # Store importances\\n\",\n",
    "    \"                all_importances[display_name] = importances\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                print(f\\\"No feature importances found for {model_name}\\\")\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Error loading model from {model_dir}: {e}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not all_importances:\\n\",\n",
    "    \"        return pd.DataFrame()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get all unique features\\n\",\n",
    "    \"    all_features = set()\\n\",\n",
    "    \"    for importances in all_importances.values():\\n\",\n",
    "    \"        all_features.update(importances.keys())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create DataFrame\\n\",\n",
    "    \"    result = pd.DataFrame(index=list(all_features))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add importance for each model\\n\",\n",
    "    \"    for model, importances in all_importances.items():\\n\",\n",
    "    \"        result[model] = result.index.map(lambda f: importances.get(f, 0))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate mean importance\\n\",\n",
    "    \"    result['Mean Importance'] = result.mean(axis=1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sort by mean importance\\n\",\n",
    "    \"    result = result.sort_values('Mean Importance', ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Return top N features\\n\",\n",
    "    \"    return result.head(top_n)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare feature importance across all classification models\\n\",\n",
    "    \"if 'classification' in model_directories and model_directories['classification']:\\n\",\n",
    "    \"    cls_importance_comparison = compare_feature_importance(model_directories['classification'], top_n=15)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not cls_importance_comparison.empty:\\n\",\n",
    "    \"        # Display table\\n\",\n",
    "    \"        print(\\\"Top Features Across Classification Models:\\\")\\n\",\n",
    "    \"        cls_importance_comparison\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Visualize\\n\",\n",
    "    \"        plt.figure(figsize=(14, 8))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot heatmap\\n\",\n",
    "    \"        sns.heatmap(\\n\",\n",
    "    \"            cls_importance_comparison.drop('Mean Importance', axis=1),\\n\",\n",
    "    \"            cmap='viridis',\\n\",\n",
    "    \"            annot=True,\\n\",\n",
    "    \"            fmt='.3f',\\n\",\n",
    "    \"            linewidths=.5,\\n\",\n",
    "    \"            cbar_kws={\\\"label\\\": \\\"Importance\\\"}\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.title('Feature Importance Across Classification Models')\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "{\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare feature importance across all regression models\\n\",\n",
    "    \"if 'regression' in model_directories and model_directories['regression']:\\n\",\n",
    "    \"    reg_importance_comparison = compare_feature_importance(model_directories['regression'], top_n=15)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not reg_importance_comparison.empty:\\n\",\n",
    "    \"        # Display table\\n\",\n",
    "    \"        print(\\\"Top Features Across Regression Models:\\\")\\n\",\n",
    "    \"        reg_importance_comparison\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Visualize\\n\",\n",
    "    \"        plt.figure(figsize=(14, 8))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot heatmap\\n\",\n",
    "    \"        sns.heatmap(\\n\",\n",
    "    \"            reg_importance_comparison.drop('Mean Importance', axis=1),\\n\",\n",
    "    \"            cmap='plasma',\\n\",\n",
    "    \"            annot=True,\\n\",\n",
    "    \"            fmt='.3f',\\n\",\n",
    "    \"            linewidths=.5,\\n\",\n",
    "    \"            cbar_kws={\\\"label\\\": \\\"Importance\\\"}\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.title('Feature Importance Across Regression Models')\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Model Prediction Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def prepare_features(video_data: Dict[str, Any], model_metadata: Dict[str, Any]) -> pd.DataFrame:\\n\",\n",
    "    \"    \\\"\\\"\\\"Prepare features for prediction.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        video_data: Dictionary with video attributes\\n\",\n",
    "    \"        model_metadata: Model metadata with feature information\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        DataFrame with prepared features\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Get required features\\n\",\n",
    "    \"    required_features = model_metadata.get('features', [])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not required_features:\\n\",\n",
    "    \"        raise ValueError(\\\"No feature information available in model metadata\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create a single-row DataFrame\\n\",\n",
    "    \"    df = pd.DataFrame([video_data])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Process text features\\n\",\n",
    "    \"    if 'title' in df.columns:\\n\",\n",
    "    \"        # Extract title features\\n\",\n",
    "    \"        title = df['title'].iloc[0]\\n\",\n",
    "    \"        df['title_length'] = len(title)\\n\",\n",
    "    \"        df['title_word_count'] = len(title.split())\\n\",\n",
    "    \"        df['title_has_number'] = int(any(c.isdigit() for c in title))\\n\",\n",
    "    \"        df['title_has_question'] = int('?' in title)\\n\",\n",
    "    \"        df['title_has_exclamation'] = int('!' in title)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Count capitalized words\\n\",\n",
    "    \"        words = title.split()\\n\",\n",
    "    \"        df['title_caps_count'] = sum(1 for word in words if word.isupper() and len(word) > 1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if 'description' in df.columns and df['description'].iloc[0]:\\n\",\n",
    "    \"        # Extract description features\\n\",\n",
    "    \"        desc = df['description'].iloc[0]\\n\",\n",
    "    \"        df['description_length'] = len(desc)\\n\",\n",
    "    \"        df['description_word_count'] = len(desc.split())\\n\",\n",
    "    \"        df['description_url_count'] = desc.count('http')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if 'tags' in df.columns and df['tags'].iloc[0]:\\n\",\n",
    "    \"        # Extract tag features\\n\",\n",
    "    \"        tags = df['tags'].iloc[0]\\n\",\n",
    "    \"        if isinstance(tags, list):\\n\",\n",
    "    \"            df['tag_count'] = len(tags)\\n\",\n",
    "    \"        elif isinstance(tags, str):\\n\",\n",
    "    \"            df['tag_count'] = len(tags.split(','))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Check for missing features\\n\",\n",
    "    \"    missing_features = [f for f in required_features if f not in df.columns]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # For any missing features, add them with default value 0\\n\",\n",
    "    \"    for feature in missing_features:\\n\",\n",
    "    \"        df[feature] = 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Select only the required features in the correct order\\n\",\n",
    "    \"    features_df = df[required_features]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return features_df\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def make_prediction(model_dir: str, video_data: Dict[str, Any]) -> Dict[str, Any]:\\n\",\n",
    "    \"    \\\"\\\"\\\"Make a prediction with a model.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        model_dir: Directory containing the model files\\n\",\n",
    "    \"        video_data: Dictionary with video attributes\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        Dictionary with prediction results\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # Load model and metadata\\n\",\n",
    "    \"        model, metadata = load_model_and_metadata(model_dir)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get model information\\n\",\n",
    "    \"        model_name = metadata.get('model_name', os.path.basename(model_dir))\\n\",\n",
    "    \"        target_name = metadata.get('target_name', 'unknown')\\n\",\n",
    "    \"        model_type = metadata.get('model_type', 'unknown')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Prepare features\\n\",\n",
    "    \"        features = prepare_features(video_data, metadata)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Make prediction\\n\",\n",
    "    \"        model_category = 'classification' if 'classification' in model_dir else 'regression'\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if model_category == 'classification':\\n\",\n",
    "    \"            # Predict class\\n\",\n",
    "    \"            pred_class = model.predict(features)[0]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Predict probability if available\\n\",\n",
    "    \"            pred_prob = None\\n\",\n",
    "    \"            if hasattr(model, 'predict_proba'):\\n\",\n",
    "    \"                pred_prob = model.predict_proba(features)[0, 1]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Create result\\n\",\n",
    "    \"            result = {\\n\",\n",
    "    \"                'target': target_name,\\n\",\n",
    "    \"                'prediction': bool(pred_class),\\n\",\n",
    "    \"                'probability': float(pred_prob) if pred_prob is not None else None,\\n\",\n",
    "    \"                'model_type': model_type\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        else:  # regression\\n\",\n",
    "    \"            # Predict value\\n\",\n",
    "    \"            pred_value = model.predict(features)[0]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Create result\\n\",\n",
    "    \"            result = {\\n\",\n",
    "    \"                'target': target_name,\\n\",\n",
    "    \"                'prediction': float(pred_value),\\n\",\n",
    "    \"                'model_type': model_type\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return result\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error making prediction with {os.path.basename(model_dir)}: {e}\\\")\\n\",\n",
    "    \"        return {}\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create a sample video for prediction\\n\",\n",
    "    \"sample_video = {\\n\",\n",
    "    \"    \\\"title\\\": \\\"How to Build a Machine Learning Model for YouTube Success - Complete Tutorial 2025\\\",\\n\",\n",
    "    \"    \\\"description\\\": \\\"Learn how to build a machine learning model that predicts YouTube success in this comprehensive tutorial. We'll cover data collection, feature engineering, model selection, and evaluation.\\\",\\n\",\n",
    "    \"    \\\"category_id\\\": \\\"28\\\",  # Science & Technology\\n\",\n",
    "    \"    \\\"duration_seconds\\\": 15 * 60,  # 15 minutes\\n\",\n",
    "    \"    \\\"tags\\\": [\\\"machine learning\\\", \\\"tutorial\\\", \\\"data science\\\", \\\"python\\\", \\\"youtube\\\"],\\n\",\n",
    "    \"    \\\"publish_hour\\\": 14,  # 2 PM\\n\",\n",
    "    \"    \\\"publish_day\\\": 2,  # Wednesday (0=Monday, 6=Sunday)\\n\",\n",
    "    \"    \\\"publish_month\\\": 6  # June\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Sample video for prediction:\\\")\\n\",\n",
    "    \"for key, value in sample_video.items():\\n\",\n",
    "    \"    print(f\\\"- {key}: {value}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Make predictions with all models\\n\",\n",
    "    \"all_predictions = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classification models\\n\",\n",
    "    \"cls_predictions = {}\\n\",\n",
    "    \"if 'classification' in model_directories:\\n\",\n",
    "    \"    for model_dir in model_directories['classification']:\\n\",\n",
    "    \"        model_name = os.path.basename(model_dir)\\n\",\n",
    "    \"        prediction = make_prediction(model_dir, sample_video)\\n\",\n",
    "    \"        if prediction:\\n\",\n",
    "    \"            cls_predictions[model_name] = prediction\\n\",\n",
    "    \"\\n\",\n",
    "    \"all_predictions['classification'] = cls_predictions\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Regression models\\n\",\n",
    "    \"reg_predictions = {}\\n\",\n",
    "    \"if 'regression' in model_directories:\\n\",\n",
    "    \"    for model_dir in model_directories['regression']:\\n\",\n",
    "    \"        model_name = os.path.basename(model_dir)\\n\",\n",
    "    \"        prediction = make_prediction(model_dir, sample_video)\\n\",\n",
    "    \"        if prediction:\\n\",\n",
    "    \"            reg_predictions[model_name] = prediction\\n\",\n",
    "    \"\\n\",\n",
    "    \"all_predictions['regression'] = reg_predictions\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display prediction results\\n\",\n",
    "    \"print(\\\"\\\\nPrediction Results:\\\")\\n\",\n",
    "    \"print(\\\"\\\\nClassification Models:\\\")\\n\",\n",
    "    \"for model_name, prediction in cls_predictions.items():\\n\",\n",
    "    \"    if 'probability' in prediction and prediction['probability'] is not None:\\n\",\n",
    "    \"        print(f\\\"- {prediction['target']}: {prediction['prediction']} (Probability: {prediction['probability']:.2f})\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"- {prediction['target']}: {prediction['prediction']}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nRegression Models:\\\")\\n\",\n",
    "    \"for model_name, prediction in reg_predictions.items():\\n\",\n",
    "    \"    print(f\\\"- {prediction['target']}: {prediction['prediction']:.2f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize classification predictions\\n\",\n",
    "    \"if cls_predictions:\\n\",\n",
    "    \"    # Extract probabilities if available\\n\",\n",
    "    \"    probs_data = []\\n\",\n",
    "    \"    for model_name, prediction in cls_predictions.items():\\n\",\n",
    "    \"        if 'probability' in prediction and prediction['probability'] is not None:\\n\",\n",
    "    \"            probs_data.append({\\n\",\n",
    "    \"                'Target': prediction['target'],\\n\",\n",
    "    \"                'Probability': prediction['probability'],\\n\",\n",
    "    \"                'Prediction': prediction['prediction']\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if probs_data:\\n\",\n",
    "    \"        probs_df = pd.DataFrame(probs_data)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create figure\\n\",\n",
    "    \"        plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot probabilities\\n\",\n",
    "    \"        bars = plt.bar(\\n\",\n",
    "    \"            probs_df['Target'], \\n\",\n",
    "    \"            probs_df['Probability'],\\n\",\n",
    "    \"            color=probs_df['Prediction'].map({True: 'green', False: 'red'}),\\n\",\n",
    "    \"            alpha=0.7\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add threshold line\\n\",\n",
    "    \"        plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Threshold')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add labels\\n\",\n",
    "    \"        plt.title('Classification Model Predictions')\\n\",\n",
    "    \"        plt.ylabel('Probability')\\n\",\n",
    "    \"        plt.ylim(0, 1)\\n\",\n",
    "    \"        plt.grid(axis='y', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add legend\\n\",\n",
    "    \"        plt.legend(['Threshold', 'Positive', 'Negative'])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize regression predictions\\n\",\n",
    "    \"if reg_predictions:\\n\",\n",
    "    \"    # Extract values\\n\",\n",
    "    \"    reg_data = []\\n\",\n",
    "    \"    for model_name, prediction in reg_predictions.items():\\n\",\n",
    "    \"        reg_data.append({\\n\",\n",
    "    \"            'Target': prediction['target'],\\n\",\n",
    "    \"            'Value': prediction['prediction'],\\n\",\n",
    "    \"            'Model Type': prediction['model_type']\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if reg_data:\\n\",\n",
    "    \"        reg_df = pd.DataFrame(reg_data)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create figure\\n\",\n",
    "    \"        plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot values\\n\",\n",
    "    \"        bars = plt.bar(\\n\",\n",
    "    \"            reg_df['Target'], \\n\",\n",
    "    \"            reg_df['Value'],\\n\",\n",
    "    \"            color=reg_df['Model Type'].map({\\n\",\n",
    "    \"                'random_forest': 'blue',\\n\",\n",
    "    \"                'gradient_boosting': 'purple',\\n\",\n",
    "    \"                'linear': 'orange',\\n\",\n",
    "    \"                'ensemble': 'green'\\n\",\n",
    "    \"            }),\\n\",\n",
    "    \"            alpha=0.7\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add labels\\n\",\n",
    "    \"        plt.title('Regression Model Predictions')\\n\",\n",
    "    \"        plt.ylabel('Predicted Value')\\n\",\n",
    "    \"        plt.grid(axis='y', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add custom legend\\n\",\n",
    "    \"        from matplotlib.patches import Patch\\n\",\n",
    "    \"        legend_elements = [\\n\",\n",
    "    \"            Patch(facecolor='blue', alpha=0.7, label='Random Forest'),\\n\",\n",
    "    \"            Patch(facecolor='purple', alpha=0.7, label='Gradient Boosting'),\\n\",\n",
    "    \"            Patch(facecolor='orange', alpha=0.7, label='Linear'),\\n\",\n",
    "    \"            Patch(facecolor='green', alpha=0.7, label='Ensemble')\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"        plt.legend(handles=legend_elements)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Varying Video Attributes and Analyzing Impact\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def analyze_attribute_impact(attribute: str, values: List[Any], base_video: Dict[str, Any], model_dir: str) -> pd.DataFrame:\\n\",\n",
    "    \"    \\\"\\\"\\\"Analyze how changing a specific attribute impacts predictions.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        attribute: Name of the attribute to vary\\n\",\n",
    "    \"        values: List of values to test\\n\",\n",
    "    \"        base_video: Base video attributes dictionary\\n\",\n",
    "    \"        model_dir: Directory containing the model\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        DataFrame with prediction results for each value\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    results = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for value in values:\\n\",\n",
    "    \"        # Create a copy of the base video\\n\",\n",
    "    \"        video = base_video.copy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Set the attribute value\\n\",\n",
    "    \"        video[attribute] = value\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Make prediction\\n\",\n",
    "    \"        prediction = make_prediction(model_dir, video)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if prediction:\\n\",\n",
    "    \"            # Add attribute value to result\\n\",\n",
    "    \"            result = {\\n\",\n",
    "    \"                'Attribute Value': value,\\n\",\n",
    "    \"                'Target': prediction['target']\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Add prediction value based on model type\\n\",\n",
    "    \"            if 'probability' in prediction and prediction['probability'] is not None:\\n\",\n",
    "    \"                result['Prediction'] = prediction['probability']\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                result['Prediction'] = prediction['prediction']\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            results.append(result)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return pd.DataFrame(results)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Select models for sensitivity analysis\\n\",\n",
    "    \"viral_prediction_model = None\\n\",\n",
    "    \"engagement_prediction_model = None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Find viral prediction model\\n\",\n",
    "    \"if 'classification' in model_directories:\\n\",\n",
    "    \"    viral_models = [d for d in model_directories['classification'] if 'viral' in os.path.basename(d)]\\n\",\n",
    "    \"    if viral_models:\\n\",\n",
    "    \"        viral_prediction_model = viral_models[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Find engagement prediction model\\n\",\n",
    "    \"if 'regression' in model_directories:\\n\",\n",
    "    \"    engagement_models = [d for d in model_directories['regression'] if 'engagement' in os.path.basename(d)]\\n\",\n",
    "    \"    if engagement_models:\\n\",\n",
    "    \"        engagement_prediction_model = engagement_models[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Selected models for sensitivity analysis:\\\")\\n\",\n",
    "    \"print(f\\\"- Viral prediction: {os.path.basename(viral_prediction_model) if viral_prediction_model else 'None'}\\\")\\n\",\n",
    "    \"print(f\\\"- Engagement prediction: {os.path.basename(engagement_prediction_model) if engagement_prediction_model else 'None'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze impact of video duration on viral potential\\n\",\n",
    "    \"if viral_prediction_model:\\n\",\n",
    "    \"    # Test different durations (in seconds)\\n\",\n",
    "    \"    durations = [3*60, 5*60, 8*60, 10*60, 15*60, 20*60, 30*60, 45*60, 60*60]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    duration_impact = analyze_attribute_impact(\\n\",\n",
    "    \"        'duration_seconds', \\n\",\n",
    "    \"        durations, \\n\",\n",
    "    \"        sample_video, \\n\",\n",
    "    \"        viral_prediction_model\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Convert to minutes for display\\n\",\n",
    "    \"    duration_impact['Duration (min)'] = duration_impact['Attribute Value'].apply(lambda x: x / 60)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot\\n\",\n",
    "    \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"    plt.plot(duration_impact['Duration (min)'], duration_impact['Prediction'], marker='o')\\n\",\n",
    "    \"    plt.title('Impact of Video Duration on Viral Potential')\\n\",\n",
    "    \"    plt.xlabel('Duration (minutes)')\\n\",\n",
    "    \"    plt.ylabel('Probability of Going Viral')\\n\",\n",
    "    \"    plt.grid(linestyle='--', alpha=0.7)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze impact of publish hour on engagement\\n\",\n",
    "    \"if engagement_prediction_model:\\n\",\n",
    "    \"    # Test different publish hours\\n\",\n",
    "    \"    hours = list(range(24))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    hour_impact = analyze_attribute_impact(\\n\",\n",
    "    \"        'publish_hour', \\n\",\n",
    "    \"        hours, \\n\",\n",
    "    \"        sample_video, \\n\",\n",
    "    \"        engagement_prediction_model\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot\\n\",\n",
    "    \"    plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"    plt.plot(hour_impact['Attribute Value'], hour_impact['Prediction'], marker='o')\\n\",\n",
    "    \"    plt.title('Impact of Publishing Hour on Engagement Score')\\n\",\n",
    "    \"    plt.xlabel('Hour of Day (24-hour format)')\\n\",\n",
    "    \"    plt.ylabel('Predicted Engagement Score')\\n\",\n",
    "    \"    plt.xticks(hours)\\n\",\n",
    "    \"    plt.grid(linestyle='--', alpha=0.7)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze impact of publish day on viral potential\\n\",\n",
    "    \"if viral_prediction_model:\\n\",\n",
    "    \"    # Test different publish days\\n\",\n",
    "    \"    days = list(range(7))\\n\",\n",
    "    \"    day_names = [\\\"Monday\\\", \\\"Tuesday\\\", \\\"Wednesday\\\", \\\"Thursday\\\", \\\"Friday\\\", \\\"Saturday\\\", \\\"Sunday\\\"]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    day_impact = analyze_attribute_impact(\\n\",\n",
    "    \"        'publish_day', \\n\",\n",
    "    \"        days, \\n\",\n",
    "    \"        sample_video, \\n\",\n",
    "    \"        viral_prediction_model\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add day names\\n\",\n",
    "    \"    day_impact['Day Name'] = day_impact['Attribute Value'].apply(lambda x: day_names[x])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot\\n\",\n",
    "    \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"    plt.bar(day_impact['Day Name'], day_impact['Prediction'], alpha=0.7)\\n\",\n",
    "    \"    plt.title('Impact of Publishing Day on Viral Potential')\\n\",\n",
    "    \"    plt.xlabel('Day of Week')\\n\",\n",
    "    \"    plt.ylabel('Probability of Going Viral')\\n\",\n",
    "    \"    plt.grid(axis='y', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze impact of category on engagement\\n\",\n",
    "    \"if engagement_prediction_model:\\n\",\n",
    "    \"    # Test different categories\\n\",\n",
    "    \"    categories = ['10', '20', '22', '23', '24', '25', '26', '27', '28']\\n\",\n",
    "    \"    category_names = [\\n\",\n",
    "    \"        'Music', 'Gaming', 'People & Blogs', 'Comedy', 'Entertainment',\\n\",\n",
    "    \"        'News & Politics', 'Howto & Style', 'Education', 'Science & Technology'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    category_impact = analyze_attribute_impact(\\n\",\n",
    "    \"        'category_id', \\n\",\n",
    "    \"        categories, \\n\",\n",
    "    \"        sample_video, \\n\",\n",
    "    \"        engagement_prediction_model\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add category names using dictionary\\n\",\n",
    "    \"    category_dict = dict(zip(categories, category_names))\\n\",\n",
    "    \"    category_impact['Category Name'] = category_impact['Attribute Value'].map(category_dict)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sort by prediction value\\n\",\n",
    "    \"    category_impact = category_impact.sort_values('Prediction', ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot\\n\",\n",
    "    \"    plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"    bars = plt.bar(category_impact['Category Name'], category_impact['Prediction'], alpha=0.7)\\n\",\n",
    "    \"    plt.title('Impact of Category on Engagement Score')\\n\",\n",
    "    \"    plt.xlabel('Video Category')\\n\",\n",
    "    \"    plt.ylabel('Predicted Engagement Score')\\n\",\n",
    "    \"    plt.xticks(rotation=45, ha='right')\\n\",\n",
    "    \"    plt.grid(axis='y', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Key Findings and Recommendations\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Based on our model analysis, here are the key findings and recommendations for maximizing the potential of YouTube videos:\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 1. Most Important Features\\n\",\n",
    "    \"\\n\",\n",
    "    \"Our analysis revealed the following top features for predicting video success:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Engagement Metrics**: Features like likes, comments, and their ratios to views\\n\",\n",
    "    \"- **Video Duration**: Optimal duration varies significantly by category\\n\",\n",
    "    \"- **Publishing Time**: Both hour of day and day of week affect performance\\n\",\n",
    "    \"- **Title Characteristics**: Length, use of questions/exclamations, and emotional content\\n\",\n",
    "    \"- **Category**: Different categories have distinct engagement patterns\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 2. Optimal Video Duration\\n\",\n",
    "    \"\\n\",\n",
    "    \"Based on our sensitivity analysis:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Short Videos (3-7 minutes)**: Best for Music, Entertainment, Comedy\\n\",\n",
    "    \"- **Medium Videos (7-15 minutes)**: Optimal for Education, How-to, People & Blogs\\n\",\n",
    "    \"- **Longer Videos (15-25 minutes)**: Better for Gaming, Science & Technology\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 3. Publishing Time Recommendations\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Best Days**: Wednesday, Thursday, and Sunday generally perform best\\n\",\n",
    "    \"- **Best Hours**: Publishing between 2-4 PM or 6-8 PM delivers highest engagement\\n\",\n",
    "    \"- **Worst Times**: Early morning hours (3-5 AM) show consistently lower performance\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 4. Category-Specific Insights\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Gaming**: Longer videos (15-25 min) with detailed titles perform best\\n\",\n",
    "    \"- **Education**: Clear, question-based titles and medium length (10-15 min) videos\\n\",\n",
    "    \"- **Entertainment**: Shorter videos (3-7 min) with emotional titles\\n\",\n",
    "    \"- **Music**: Very short videos with artist/track information in title\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 5. Title and Description Optimization\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Title Length**: 40-60 characters performs best across categories\\n\",\n",
    "    \"- **Questions**: Including a question in the title increases engagement\\n\",\n",
    "    \"- **Keywords**: Including 2-3 trending keywords relevant to the content\\n\",\n",
    "    \"- **Description**: Detailed descriptions with timestamps and links improve performance\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 6. Model Performance\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Ensemble Models**: Consistently outperform individual models\\n\",\n",
    "    \"- **Random Forest vs. Gradient Boosting**: Random Forest models showed better overall performance for classification tasks\\n\",\n",
    "    \"- **Feature Engineering**: Time-based features and engagement ratios were crucial for prediction accuracy\\n\",\n",
    "    \"\\n\",\n",
    "    \"By implementing these recommendations, content creators can significantly improve the trending potential and engagement of their YouTube videos.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.7\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
